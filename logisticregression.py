# -*- coding: utf-8 -*-
"""LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WaHZXHV0kFf9YKGZ3Zuxho0cc_HaCn9Q
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/Titanic-Dataset.csv')
df.head()

# checking total null values
df.isnull().sum()

# replacing null values in age with median
median_value = df['Age'].median()
df['Age'].fillna(median_value, inplace=True)

# dropping unnecessary columns
df.drop('Cabin', axis=1, inplace=True)
df.drop('PassengerId', axis=1, inplace=True)
df.drop('Name', axis=1, inplace=True)
df.drop('Ticket', axis=1, inplace=True)

# there were only 2 nulls in the Embarked columns so filling them with the mode of that column
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

# after removing all the nulls checking if there is any null value left
df.isnull().sum()

# Encodeing the embarked column using OneHotEncoder
from sklearn.preprocessing import OneHotEncoder

# Initialize encoder with sparse_output=False
Encoder = OneHotEncoder(sparse_output=False, drop=None)

# Fit and transform
Encoded = Encoder.fit_transform(df[['Embarked']])

# Convert to DataFrame with correct column names
Encoded_df = pd.DataFrame(Encoded, columns=Encoder.get_feature_names_out(['Embarked']))

# Concatenate with original df
df = pd.concat([df, Encoded_df], axis=1)
df.head()

# after encoding the Embaarked column there is no need of the old Embarked column as 3 new Embarked columns are encoded
# dropping Embarked column
df.drop('Embarked', axis=1, inplace=True)
df.head()

# Encoding Sex column
df['Sex']=df['Sex'].map({'male':1, 'female':0 })

df.head()

# Defining Independent and Dependent features
X = df.iloc[:, 1:]
y = df.iloc[:, 0]

y

# train test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=42)

X_train

# Fit scaler on *training data only*
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)

# Transform test data using the same scaler
X_test = scaler.transform(X_test)

#  fitting Logistic Regression as the output variable(Survived) has binary outcomes (0 or 1 i.e. Yes or No)
from sklearn.linear_model import LogisticRegression

# Initializing Logistic Regression
LR = LogisticRegression(solver='saga', class_weight='balanced')

# importing GridSearchCV for hyper parameter tuning
from sklearn.model_selection import GridSearchCV

# define parameter space for hyperparameter tuning (Trial and Error method) using GridSearchCV
parameters = {'penalty':['l1', 'l2', 'elasticnet'], 'C': [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80], 'max_iter': [100,200,300], 'l1_ratio': [0.3, 0.5, 0.7]}

# feeding the above defined parameters to GridSearchCV for hyperparameter tuning
classifier_regressor = GridSearchCV(LR, parameters, scoring='recall', cv=5)

# fitting the Train data to the above defined GridSearchCV
classifier_regressor.fit(X_train, y_train)

# printing best parameters
print("Best Parameters are:",classifier_regressor.best_params_)

# printing best model score
print("Best model score is:",classifier_regressor.best_score_)

# predicting X_test
LogisticR_pred = classifier_regressor.predict(X_test)

# checking accuracy score,
from sklearn.metrics import accuracy_score, classification_report
accuracy_score(y_test, LogisticR_pred)

# checking precision and recall
print(classification_report(y_test, LogisticR_pred))

from sklearn.metrics import confusion_matrix
sns.heatmap(confusion_matrix(y_test, LogisticR_pred), annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""True Negatives (TN) = 106 → non-survivors correctly predicted.

False Positives (FP) = 28 → predicted “survived” but actually didn’t.

False Negatives (FN) = 16 → predicted “not survived” but actually survived.

True Positives (TP) = 73 → survivors correctly predicted.
"""